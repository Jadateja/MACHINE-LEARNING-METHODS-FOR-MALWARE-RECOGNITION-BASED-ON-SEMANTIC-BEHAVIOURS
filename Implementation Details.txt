Implementation is the stage of the project when the theoretical design is turned out into a working system. 
Thus, it can be considered to be the most critical stage in achieving a successful new system and in giving the user, confidence that the new system will work and be effective.
The implementation stage involves careful planning, investigation of the existing system and its constraints on implementation, designing of methods to achieve changeover and evaluation of changeover methods.

1 . Support vector classifiers algorithm:

Support Vector Machine or SVM algorithm is a simple yet powerful Supervised Machine Learning algorithm that can be used for building both regression and classification models. 
SVM algorithm can perform really well with both linearly separable and non-linearly separable datasets. Even with a limited amount of data, the support vector machine algorithm does not fail to show its magic.

Step 1: Load Pandas library and the dataset using Pandas
Step 2: Define the features and the target
Step 3: Split the dataset into train and test using sklearn before building the SVM algorithm model
Step 4: Import the support vector classifier function or SVC function from Sklearn SVM module. Build the Support Vector Machine model with the help of the SVC function
Step 5: Predict values using the SVM algorithm model
Step 6: Evaluate the Support Vector Machine model


2.  XGBoost:

XGBoost is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.
XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks. 
However, when it comes to small-to-medium structured/tabular data, decision tree based algorithms are considered best-in-class right now.

Bagging:
Now imagine instead of a single interviewer, now there is an interview panel where each interviewer has a vote.
Bagging or bootstrap aggregating involves combining inputs from all interviewers for the final decision through a democratic voting process. XGBoost and Gradient Boosting Machines (GBMs) are both ensemble tree methods that apply the principle of boosting weak learners (CARTs generally) using the gradient descent architecture. 
However, XGBoost improves upon the base GBM framework through systems optimization and algorithmic enhancements.


3.  Decision Tree:


A tree has many analogies in real life, and turns out that it has influenced a wide area of machine learning, covering both classification and regression.
In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. 
As the name goes, it uses a tree-like model of decisions. Though a commonly used tool in data mining for deriving a strategy to reach a particular goal.
A decision tree is drawn upside down with its root at the top. There are condition/internal node, based on which the tree splits into branches/ edges.
The end of the branch that doesn’t split anymore is the decision/leaf, in this case, whether the passenger died or survived, represented as red and green text respectively. 
Although, a real dataset will have a lot more features and this will just be a branch in a much bigger tree, but you can’t ignore the simplicity of this algorithm. The feature importance is clear and relations can be viewed easily. 
This methodology is more commonly known as learning decision tree from data and above tree is called Classification tree as the target is to classify passenger as survived or died. Regression trees are represented in the same manner, just they predict continuous values like price of a house.
In general, Decision Tree algorithms are referred to as CART or Classification and Regression Trees.

